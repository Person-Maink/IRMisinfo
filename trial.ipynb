{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mamay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from warcio.archiveiterator import ArchiveIterator\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from whoosh.index import create_in, open_dir, exists_in\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh.writing import AsyncWriter\n",
    "from whoosh import scoring\n",
    "from whoosh.analysis import StemmingAnalyzer, FancyAnalyzer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "import re, os\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# from pygaggle.rerank.base import Query, Text\n",
    "# from pygaggle.rerank.transformer import MonoT5\n",
    "\n",
    "if not os.path.exists(\"indexdir2\"):\n",
    "    os.mkdir(\"indexdir2\")\n",
    "\n",
    "# schema = Schema(docid=ID(stored=True), title=ID(stored=True), content=TEXT(stored=True, analyzer=FancyAnalyzer()))\n",
    "schema = Schema(docid=ID(stored=True), title=ID(stored=True), content=TEXT(stored=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting WARC documents: 37341it [06:32, 95.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 37340 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting WARC documents: 37167it [06:56, 89.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 37166 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting WARC documents: 37532it [07:09, 87.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 37531 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting WARC documents: 37352it [06:52, 90.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 37351 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting WARC documents: 37120it [06:41, 92.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 37119 documents.\n"
     ]
    }
   ],
   "source": [
    "def index_file(warc_file):\n",
    "    numDocs = 0\n",
    "    writer = ix.writer()\n",
    "    with gzip.open(warc_file, 'rb') as stream:\n",
    "        # for record in ArchiveIterator(stream):\n",
    "        for record in tqdm(ArchiveIterator(stream), desc=\"Extracting WARC documents\"):\n",
    "            # if numDocs >= 1000: \n",
    "            #     break\n",
    "            if record.rec_type == 'conversion':  # WET files only contain conversion records\n",
    "                url = record.rec_headers.get('WARC-Target-URI')\n",
    "                doc_id = record.rec_headers.get('WARC-Record-ID')\n",
    "                # text = word_tokenize(record.content_stream().read().decode('utf-8', errors='ignore').strip())\n",
    "                text = [t.text for t in StemmingAnalyzer()(record.content_stream().read().decode('utf-8', errors='ignore').strip())]\n",
    "\n",
    "                try:\n",
    "                    writer.add_document(docid=doc_id, title=url, content=text)\n",
    "                    numDocs += 1\n",
    "                except UnicodeEncodeError:\n",
    "                    print(f\"Non-unicode content in doc {doc_id}, skipping...\")\n",
    "\n",
    "    print(f\"Extracted {numDocs} documents.\")\n",
    "    writer.commit()\n",
    "\n",
    "ix = create_in(\"indexdir2\", schema, indexname=\"documents\")\n",
    "numDocs = 5 #max number of documents to extract/ read. MAX=32\n",
    "for i in range(1, numDocs+1): \n",
    "    # warc_file = f\"Dataset/{i}.warc\"\n",
    "    warc_file = f\"Dataset/{i}.warc.wet.gz\"\n",
    "    index_file(warc_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vitamin', 'cure', 'covid', '19']\n",
      "Can vitamin D cure COVID-19?\n",
      "(content:vitamin AND content:cure AND content:covid AND content:19)\n",
      "[]\n",
      "['vitamin', 'cure', 'covid', '19']\n",
      "Can vitamin C cure COVID-19?\n",
      "(content:vitamin AND content:cure AND content:covid AND content:19)\n",
      "[]\n",
      "['bcg', 'vaccin', 'prevent', 'covid', '19']\n",
      "Can BCG vaccine prevent COVID-19?\n",
      "(content:bcg AND content:vaccin AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['ibuprofen', 'worsen', 'covid', '19']\n",
      "Can ibuprofen worsen COVID-19?\n",
      "(content:ibuprofen AND content:worsen AND content:covid AND content:19)\n",
      "[]\n",
      "['gargl', 'salt', 'water', 'prevent', 'covid', '19']\n",
      "Can gargling salt water prevent COVID-19?\n",
      "(content:gargl AND content:salt AND content:water AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['ginger', 'cure', 'covid', '19']\n",
      "Can Ginger cure COVID-19?\n",
      "(content:ginger AND content:cure AND content:covid AND content:19)\n",
      "[]\n",
      "['5g', 'antenna', 'caus', 'covid', '19']\n",
      "Can 5G antennas cause COVID-19?\n",
      "(content:5g AND content:antenna AND content:caus AND content:covid AND content:19)\n",
      "[]\n",
      "['herbal', 'tea', 'prevent', 'covid', '19']\n",
      "Can herbal tea prevent COVID-19?\n",
      "(content:herbal AND content:tea AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['convalesc', 'plasma', 'cure', 'covid', '19']\n",
      "Can convalescent plasma cure COVID-19?\n",
      "(content:convalesc AND content:plasma AND content:cure AND content:covid AND content:19)\n",
      "[]\n",
      "['pneumococc', 'vaccin', 'prevent', 'covid', '19']\n",
      "Can pneumococcal vaccine prevent COVID-19?\n",
      "(content:pneumococc AND content:vaccin AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['femal', 'sex', 'hormon', 'prevent', 'covid', '19']\n",
      "Can female sex hormones prevent COVID-19?\n",
      "(content:femal AND content:sex AND content:hormon AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['dextromethorphan', 'worsen', 'covid', '19']\n",
      "Can dextromethorphan worsen COVID-19?\n",
      "(content:dextromethorphan AND content:worsen AND content:covid AND content:19)\n",
      "[]\n",
      "['wear', 'mask', 'prevent', 'covid', '19']\n",
      "Can wearing masks prevent COVID-19?\n",
      "(content:wear AND content:mask AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['drink', 'cow', 'urin', 'cure', 'covid', '19']\n",
      "Can drinking cow urine cure COVID-19?\n",
      "(content:drink AND content:cow AND content:urin AND content:cure AND content:covid AND content:19)\n",
      "[]\n",
      "['social', 'distanc', 'prevent', 'covid', '19']\n",
      "Can social distancing prevent COVID-19?\n",
      "(content:social AND content:distanc AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['vinegar', 'prevent', 'covid', '19']\n",
      "Can vinegar prevent COVID-19?\n",
      "(content:vinegar AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['convalesc', 'plasma', 'cure', 'covid', '19']\n",
      "Can convalescent plasma cure COVID-19?\n",
      "(content:convalesc AND content:plasma AND content:cure AND content:covid AND content:19)\n",
      "[]\n",
      "['lopinavir', 'ritonavir', 'cure', 'covid', '19']\n",
      "Can lopinavir-ritonavir cure COVID-19?\n",
      "content:['lopinavir', 'ri TO navir', 'cure', 'covid', '19']\n",
      "[{'topic_id': '18', 'doc_id': '<urn:uuid:a04e1a6c-daa0-4362-8d66-0a005ae9a984>', 'rank': 1, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:11d5cdea-480a-43c2-bb66-889b00797474>', 'rank': 2, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:316c83bf-0626-4cf5-b7c4-f21a4dc4d704>', 'rank': 3, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:58038451-499f-4907-909c-7b04a2457e36>', 'rank': 4, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:91a715cc-eeb7-41ae-b9b0-f84cd7304b4e>', 'rank': 5, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:63134250-5ddf-4906-af0c-5cd5fb37d1aa>', 'rank': 6, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:a149451b-7b71-4f39-ae31-40b2bc72acc1>', 'rank': 7, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:33b9ba5d-731c-46e2-bd7f-57d1b41274d9>', 'rank': 8, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:d20980cd-2e61-4b97-acf1-4cf6db12b4d0>', 'rank': 9, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:e12b9fb0-f3cb-4351-a16b-79bc2457d201>', 'rank': 10, 'score': 1.0}]\n",
      "['ayurveda', 'cure', 'covid', '19']\n",
      "Can ayurveda cure COVID-19?\n",
      "(content:ayurveda AND content:cure AND content:covid AND content:19)\n",
      "[]\n",
      "['exposur', 'uv', 'light', 'prevent', 'covid', '19']\n",
      "Can exposure to UV light prevent COVID-19?\n",
      "(content:exposur AND content:uv AND content:light AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['black', 'seed', 'oil', 'treat', 'covid', '19']\n",
      "Can black seed oil treat COVID-19?\n",
      "(content:black AND content:seed AND content:oil AND content:treat AND content:covid AND content:19)\n",
      "[]\n",
      "['tamiflu', 'help', 'covid', '19']\n",
      "Can Tamiflu help COVID-19?\n",
      "(content:tamiflu AND content:help AND content:covid AND content:19)\n",
      "[]\n",
      "['burn', 'neem', 'leav', 'treat', 'covid', '19']\n",
      "Can burning neem leaves treat COVID-19?\n",
      "(content:burn AND content:neem AND content:leav AND content:treat AND content:covid AND content:19)\n",
      "[]\n",
      "['drink', 'alcohol', 'worsen', 'covid', '19']\n",
      "Can drinking alcohol worsen COVID-19?\n",
      "(content:drink AND content:alcohol AND content:worsen AND content:covid AND content:19)\n",
      "[]\n",
      "['high', 'temperatur', 'humid', 'prevent', 'covid', '19']\n",
      "Can high temperatures and humidity prevent COVID-19?\n",
      "(content:high AND content:temperatur AND content:humid AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['remdesivir', 'cure', 'covid', '19']\n",
      "Can remdesivir cure COVID-19?\n",
      "(content:remdesivir AND content:cure AND content:covid AND content:19)\n",
      "[]\n",
      "['turmer', 'treat', 'covid', '19']\n",
      "Can turmeric treat COVID-19?\n",
      "(content:turmer AND content:treat AND content:covid AND content:19)\n",
      "[]\n",
      "['vegetarian', 'diet', 'prevent', 'covid', '19']\n",
      "Can a vegetarian diet prevent COVID-19?\n",
      "(content:vegetarian AND content:diet AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['gett', 'flu', 'shot', 'prevent', 'covid', '19']\n",
      "Can getting a flu shot prevent COVID-19?\n",
      "(content:gett AND content:flu AND content:shot AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['ac', 'arb', 'worsen', 'covid', '19']\n",
      "Can ACE and ARBs worsen COVID-19?\n",
      "(content:ac AND content:arb AND content:worsen AND content:covid AND content:19)\n",
      "[]\n",
      "['smoke', 'prevent', 'covid', '19']\n",
      "Can smoking prevent COVID-19?\n",
      "(content:smoke AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['have', 'type', 'blood', 'prevent', 'covid', '19']\n",
      "Can having type O blood prevent COVID-19\n",
      "(content:type AND content:blood AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['methanol', 'prevent', 'covid', '19']\n",
      "Can methanol prevent COVID-19\n",
      "(content:methanol AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['handwash', 'prevent', 'covid', '19']\n",
      "Can handwashing prevent COVID-19?\n",
      "(content:handwash AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['nicotin', 'help', 'covid', '19']\n",
      "Can Nicotine help COVID-19?\n",
      "(content:nicotin AND content:help AND content:covid AND content:19)\n",
      "[]\n",
      "['cannabi', 'help', 'covid', '19']\n",
      "Can Cannabis help COVID-19?\n",
      "(content:cannabi AND content:help AND content:covid AND content:19)\n",
      "[]\n",
      "['hydroxychloroquine', 'worsen', 'covid', '19']\n",
      "Can Hydroxychloroquine worsen COVID-19?\n",
      "(content:hydroxychloroquine AND content:worsen AND content:covid AND content:19)\n",
      "[]\n",
      "['monoclon', 'antibodi', 'cure', 'covid', '19']\n",
      "Can monoclonal antibodies cure COVID-19?\n",
      "(content:monoclon AND content:antibodi AND content:cure AND content:covid AND content:19)\n",
      "[]\n",
      "['bleach', 'prevent', 'covid', '19']\n",
      "Can bleach prevent COVID-19?\n",
      "(content:bleach AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['breast', 'milk', 'prevent', 'covid', '19']\n",
      "Can breast milk prevent COVID-19?\n",
      "(content:breast AND content:milk AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['hib', 'vaccin', 'prevent', 'covid', '19']\n",
      "Can Hib vaccine prevent COVID-19?\n",
      "(content:hib AND content:vaccin AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['inhal', 'help', 'covid', '19']\n",
      "Can inhalers help COVID-19?\n",
      "(content:inhal AND content:help AND content:covid AND content:19)\n",
      "[]\n",
      "['omega', 'prevent', 'covid', '19']\n",
      "Can Omega-3 prevent COVID-19?\n",
      "(content:omega AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['ivig', 'cure', 'covid', '19']\n",
      "Can IVIG cure COVID-19?\n",
      "(content:ivig AND content:cure AND content:covid AND content:19)\n",
      "[]\n",
      "['remestemcel', 'cure', 'covid', '19']\n",
      "Can remestemcel-l cure COVID-19?\n",
      "(content:remestemcel AND content:cure AND content:covid AND content:19)\n",
      "[]\n",
      "['mosquito', 'caus', 'covid', '19']\n",
      "Can mosquitos cause COVID-19?\n",
      "content:['mosqui TO , 'caus', 'covid', '19]\n",
      "[]\n",
      "['homemad', 'vodka', 'sanit', 'prevent', 'covid', '19']\n",
      "Can Homemade Vodka Sanitizer prevent COVID-19?\n",
      "(content:homemad AND content:vodka AND content:sanit AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['ribavirin', 'help', 'covid', '19']\n",
      "Can Ribavirin help COVID-19?\n",
      "(content:ribavirin AND content:help AND content:covid AND content:19)\n",
      "[]\n",
      "['echinacea', 'prevent', 'covid', '19']\n",
      "Can Echinacea prevent COVID-19?\n",
      "(content:echinacea AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "['drink', 'lemon', 'water', 'prevent', 'covid', '19']\n",
      "Can drinking lemon water prevent COVID-19?\n",
      "(content:drink AND content:lemon AND content:water AND content:prevent AND content:covid AND content:19)\n",
      "[]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [{'topic_id': '18', 'doc_id': '<urn:uuid:a04e1a6c-daa0-4362-8d66-0a005ae9a984>', 'rank': 1, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:11d5cdea-480a-43c2-bb66-889b00797474>', 'rank': 2, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:316c83bf-0626-4cf5-b7c4-f21a4dc4d704>', 'rank': 3, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:58038451-499f-4907-909c-7b04a2457e36>', 'rank': 4, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:91a715cc-eeb7-41ae-b9b0-f84cd7304b4e>', 'rank': 5, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:63134250-5ddf-4906-af0c-5cd5fb37d1aa>', 'rank': 6, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:a149451b-7b71-4f39-ae31-40b2bc72acc1>', 'rank': 7, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:33b9ba5d-731c-46e2-bd7f-57d1b41274d9>', 'rank': 8, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:d20980cd-2e61-4b97-acf1-4cf6db12b4d0>', 'rank': 9, 'score': 1.0}, {'topic_id': '18', 'doc_id': '<urn:uuid:e12b9fb0-f3cb-4351-a16b-79bc2457d201>', 'rank': 10, 'score': 1.0}], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "def eval_all_queries(ix, query_file=\"Dataset/queries/topics/misinfo-2020-topics.xml\", top_k=10):\n",
    "    # Build paths safely\n",
    "    query_path = os.path.join(query_file)\n",
    "\n",
    "    # Parse XML topic file\n",
    "    tree = ET.parse(query_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Initialize query parser\n",
    "    qp = QueryParser(\"content\", schema=schema)\n",
    "\n",
    "    results_all = []\n",
    "    results_reranker = []\n",
    "\n",
    "    with ix.searcher(weighting=scoring.BM25F()) as searcher:\n",
    "        for topic in root.findall(\"topic\"):\n",
    "            topic_results = []\n",
    "            topic_reranker = []\n",
    "\n",
    "            topic_id = topic.find(\"number\").text.strip()\n",
    "            description = topic.find(\"description\").text.strip()\n",
    "\n",
    "            # parsed_query = qp.parse(description)\n",
    "            # parsed_query = qp.parse(word_tokenize(description))\n",
    "            print([t.text for t in StemmingAnalyzer()(description)])\n",
    "            print(description)\n",
    "            parsed_query = qp.parse(str([t.text for t in StemmingAnalyzer()(description)]))\n",
    "            print(parsed_query)\n",
    "            results = searcher.search(parsed_query, limit=top_k)\n",
    "\n",
    "            for rank, result in enumerate(results):\n",
    "                trec_entry = {\n",
    "                    \"topic_id\": topic_id,\n",
    "                    \"doc_id\": result[\"docid\"],\n",
    "                    \"rank\": rank + 1,\n",
    "                    \"score\": result.score\n",
    "                }\n",
    "                topic_results.append(trec_entry)\n",
    "                results_reranker.append((str(rank), result['content']))\n",
    "            results_all.append(topic_results)\n",
    "            print(topic_results)\n",
    "            results_reranker.append(topic_reranker)\n",
    "\n",
    "    return results_all, results_reranker\n",
    "\n",
    "ix = open_dir(\"indexdir2\", indexname=\"documents\")\n",
    "rank_eval, rank_content = eval_all_queries(ix)\n",
    "print(rank_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored fields in document: {'content': ['20', 'minute', 'full', 'body', 'hiit', 'cardio', 'workout', 'self', 'skip', 'main', 'content', 'open', 'navigation', 'menu', 'menu', 'full', 'body', 'hiit', 'cardio', 'workout', 'no', 'equipment', 'fitness', 'food', 'health', 'love', 'beauty', 'culture', 'more', 'chevron', 'search', 'search', 'fitness', 'workouts', 'shape', 'up', 'running', 'yoga', 'food', 'healthy', 'eating', 'nutrition', 'weight', 'loss', 'recipes', 'cooking', 'health', 'mental', 'health', 'sexual', 'reproductive', 'health', 'pregnancy', 'motherhood', 'sleep', 'love', 'sex', 'relationships', 'weddings', 'single', 'life', 'breakups', 'beauty', 'makeup', 'hair', 'fashion', 'nails', 'skin', 'culture', 'career', 'money', 'travel', 'entertainment', 'technology', 'family', 'sports', 'politics', 'fitness', 'december', '31', '2019', 'full', 'body', 'hiit', 'cardio', 'workout', 'no', 'equipment', 'goal', 'today’s', 'workout', 'simple', 'get', 'breathless', 'amy', 'eisinger', 'facebook', 'twitter', 'pinterest', 'katie', 'thompson', 'facebook', 'twitter', 'pinterest', 'you’ve', 'got', 'about', '20', 'minutes', 'little', 'bit', 'space', 'do', 'today’s', 'workout', 'no', 'equipment', 'routine', 'hiit', 'cardio', 'workout', 'so', 'it’s', 'all', 'about', 'spiking', 'heart', 'rate', 'moving', 'quickly', 'safely', 'don’t', 'worry—we', 'still', 'kept', 'fairly', 'low', 'impact', 'one', 'move', 'requires', 'some', 'hopping', 'skater', 'simply', 'take', 'large', 'steps', 'rather', 'than', 'jump', 'we’ve', 'detailed', 'modification', 'below', 'keep', 'mind', 'work', 'though', 'moves', 'like', 'reverse', 'lunges', 'should', 'still', 'engaging', 'core', 'throughout', 'even', 'though', 'it’s', 'strict', '“core”', 'move', 'doesn’t', 'mean', 'should', 'ignore', 'core', 'spinal', 'alignment', 'before', 'any', 'hiit', 'cardio', 'workout', 'it’s', 'always', 'good', 'idea', 'do', 'warm', 'up', 'could', 'try', 'either', 'quick', 'routine', 'these', 'bodyweight', 'moves', 'both', 'which', 'come', 'previous', 'challenges', 'general', 'dynamic', 'warm', 'ups—where', 'continuously', 'move', 'through', 'gentle', 'movements—are', 'better', 'idea', 'than', 'static', 'stretching', 'because', 'they', 'help', 'reduce', 'risk', 'injury', 'they', 'increase', 'mobility', 'while', 'know', 'you’re', 'busy—and', 'tempting', 'skip', 'warm', 'up', 'entirely—you', 'really', 'shouldn’t', 'actually', 'set', 'yourself', 'up', 'less', 'effective', 'entire', 'workout', 'skipping', 'warm', 'up', 'so', 'take', 'those', 'five', 'minutes', 'extra', 'body', 'thank', 'hiit', 'cardio', 'workout', 'below', 'day', 'challenge', 'check', 'out', 'full', 'month', 'workouts', 'right', 'here', 'go', 'workout', 'calendar', 'here', 'workout', 'directions', 'do', 'each', 'move', 'below', 'selected', 'period', 'work', 'rest', 'time', 'option', 'after', 'last', 'move', 'rest', '60', 'seconds', 'circuit', 'do', 'entire', 'circuit', 'times', 'after', 'last', 'circuit', 'try', 'bonus', 'move', '60', 'seconds', 'option', '30', 'seconds', 'work', '30', 'seconds', 'rest', 'option', '40', 'seconds', 'work', '20', 'seconds', 'rest', 'option', '50', 'seconds', 'work', '10', 'seconds', 'rest', 'pinterest', 'katie', 'thompson', 'skater', 'stand', 'feet', 'hip', 'width', 'apart', 'lift', 'right', 'leg', 'jump', 'right', 'let', 'left', 'leg', 'straighten', 'follow', 'land', 'right', 'foot', 'swing', 'left', 'foot', 'behind', 'tap', 'left', 'foot', 'floor', 'option', 'bring', 'left', 'hand', 'down', 'tap', 'floor', 'allow', 'right', 'arm', 'swing', 'behind', 'back', 'swing', 'left', 'leg', 'left', 'jump', 'landing', 'lightly', 'left', 'foot', 'allowing', 'right', 'foot', 'swing', 'behind', 'right', 'fingertips', 'come', 'down', 'toward', 'floor', 'continue', 'skate', 'side', 'side', 'make', 'easier', 'rather', 'than', 'jumping', 'side', 'side', 'make', 'move', 'low', 'impact', 'taking', 'big', 'steps', 'side', 'side', 'pinterest', 'katie', 'thompson', 'bicycle', 'crunch', 'lie', 'faceup', 'legs', 'tabletop', 'position', 'knees', 'bent', '90', 'degrees', 'stacked', 'over', 'hips', 'place', 'hands', 'behind', 'head', 'elbows', 'bent', 'pointing', 'out', 'sides', 'use', 'abs', 'curl', 'shoulders', 'off', 'floor', 'starting', 'position', 'twist', 'bring', 'right', 'elbow', 'left', 'knee', 'while', 'simultaneously', 'straightening', 'right', 'leg', 'then', 'twist', 'bring', 'left', 'elbow', 'right', 'knee', 'simultaneously', 'straightening', 'left', 'leg', 'continue', 'alternating', 'sides', 'go', 'slow', 'steady', 'pace', 'so', 'really', 'twist', 'feel', 'abs', 'working', 'pinterest', 'katie', 'thompson', 'reverse', 'lunge', 'alternating', 'sides', 'stand', 'feet', 'shoulder', 'width', 'apart', 'core', 'engaged', 'step', 'back', 'right', 'foot', 'bend', 'both', 'knees', 'sink', 'into', 'lunge', 'keep', 'core', 'engaged', 'hips', 'tucked', 'back', 'straight', 'return', 'starting', 'position', 'pushing', 'off', 'right', 'foot', 'stepping', 'forward', 'repeat', 'other', 'side', 'continue', 'alternate', 'sides', 'make', 'harder', 'hold', 'dumbbell', 'each', 'hand', 'pinterest', 'katie', 'thompson', 'tuck', 'up', 'lie', 'faceup', 'legs', 'extended', 'arms', 'sides', 'engage', 'core', 'lift', 'both', 'arms', 'legs', 'few', 'inches', 'off', 'floor', 'come', 'into', 'hollow', 'hold', 'position', 'now', 'crunch', 'up', 'bringing', 'knees', 'chest', 'wrapping', 'hands', 'lightly', 'around', 'shins', 'keep', 'core', 'tight', 'balance', 'sit', 'bones—do', 'grip', 'shins', 'hug', 'knees', 'order', 'achieve', 'balance', 'lower', 'return', 'hollow', 'hold', 'position', 'repeat', 'pinterest', 'katie', 'thompson', 'bonus', 'move', 'bird', 'dog', 'crunch', 'after', 'last', 'circuit', 'try', 'bonus', 'move', '60', 'seconds', 'start', 'hands', 'knees', 'tabletop', 'position', 'wrists', 'stacked', 'under', 'shoulders', 'knees', 'stacked', 'under', 'hips', 'extend', 'right', 'arm', 'forward', 'left', 'leg', 'back', 'maintaining', 'flat', 'back', 'keeping', 'hips', 'line', 'floor', 'think', 'about', 'driving', 'foot', 'toward', 'wall', 'behind', 'squeeze', 'abs', 'draw', 'right', 'elbow', 'left', 'knee', 'meet', 'near', 'center', 'body', 'reverse', 'movement', 'extend', 'arm', 'leg', 'back', 'out', 'continue', 'movement', '30', 'seconds', 'same', 'side', 'then', 'switch', 'other', 'side', 'another', '30', 'seconds', 'pinterest', 'katie', 'thompson', 'morgan', 'johnson', 'pin', 'workout', 'all', 'images', 'gifs', 'photographer', 'katie', 'thompson', 'stylist', 'rika', 'watanabe', 'hair', 'hide', 'suzuki', 'makeup', 'rachel', 'ghorbani', 'wardrobe', 'still', 'images', 'our', 'athlete', 'la', 'toneya', 'burwell', 'wears', 'athleta', 'hyper', 'focused', 'chroma', 'bra', '45', 'athleta', 'com', 'target', 'leggings', 'similar', 'styles', 'target', 'com', 'apl', 'women’s', 'tech', 'loom', 'pro', 'sneakers', '140', 'athleticpropulsionlabs', 'com', 'workout', 'gifs', 'la', 'toneya', 'wears', 'lululemon', 'bra', 'similar', 'styles', 'lululemon', 'com', 'beyond', 'yoga', 'leggings', 'similar', 'styles', 'beyondyoga', 'com', 'new', 'balance', 'fresh', 'foam', 'lazr', 'hypo', 'knit', 'sneakers', '100', 'newbalance', 'com', 'workout', 'gif', 'our', 'athlete', 'mars', 'dixon', 'wears', 'lululemon', 'shirt', 'similar', 'styles', 'lululemon', 'com', 'outdoor', 'voices', 'rec', 'shorts', '55', 'outdoorvoices', 'com', 'reebok', 'sneakers', 'similar', 'styles', 'reebok', 'com', 'all', 'products', 'featured', 'self', 'independently', 'selected', 'our', 'editors', 'buy', 'something', 'through', 'our', 'retail', 'links', 'earn', 'affiliate', 'commission', 'amy', 'certified', 'personal', 'trainer', 'pronatal', 'prenatal', 'postpartum', 'certified', 'host', 'trainer', 'sweat', 'self', 'workout', 'videos', 'she', 'native', 'floridian', 'who', 'has', 'been', 'living', 'new', 'york', 'city', 'roughly', 'decade', 'writing', 'editing', 'doing', 'all', 'things', 'digital', 'she', 'received', 'her', 'read', 'more', 'instagram', 'facebook', 'self', 'does', 'provide', 'medical', 'advice', 'diagnosis', 'treatment', 'any', 'information', 'published', 'website', 'brand', 'intended', 'substitute', 'medical', 'advice', 'should', 'take', 'any', 'action', 'before', 'consulting', 'healthcare', 'professional', 'topics', 'fitness', 'challengenew', 'years', 'challenge', 'workoutfitness', 'hiit', 'workoutsno', 'equipment', 'discover', 'new', 'workout', 'ideas', 'healthy', 'eating', 'recipes', 'makeup', 'looks', 'skin', 'care', 'advice', 'best', 'beauty', 'products', 'tips', 'trends', 'more', 'self', 'facebook', 'twitter', 'pinterest', 'instagram', 'more', 'self', 'about', 'self', 'newsletter', 'sign', 'up', 'video', 'masthead', 'accessibility', 'help', 'contact', 'contact', 'subscription', 'services', 'careers', 'rss', 'feeds', 'site', 'map', 'accessibility', 'help', 'condé', 'nast', 'store', '2020', 'condé', 'nast', 'all', 'rights', 'reserved', 'use', 'site', 'constitutes', 'acceptance', 'our', 'user', 'agreement', 'updated', '20', 'privacy', 'policy', 'cookie', 'statement', 'updated', '20', 'california', 'privacy', 'rights', 'do', 'sell', 'my', 'personal', 'information', 'self', 'earn', 'portion', 'sales', 'products', 'purchased', 'through', 'our', 'site', 'part', 'our', 'affiliate', 'partnerships', 'retailers', 'material', 'site', 'reproduced', 'distributed', 'transmitted', 'cached', 'otherwise', 'used', 'except', 'prior', 'written', 'permission', 'condé', 'nast', 'ad', 'choices'], 'docid': '<urn:uuid:63134250-5ddf-4906-af0c-5cd5fb37d1aa>', 'title': 'https://www.self.com/gallery/no-equipment-hiit-cardio-workout'}\n"
     ]
    }
   ],
   "source": [
    "ix = open_dir(\"indexdir\", indexname=\"documents\")\n",
    "with ix.searcher() as searcher:\n",
    "    docnum = 5  # change this to the document number you want\n",
    "    stored_fields = searcher.stored_fields(docnum)\n",
    "    print(\"Stored fields in document:\", stored_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_rerank_all_queries(rank_eval, rank_content):\n",
    "    reranker = MonoT5()\n",
    "    reranked_all = []\n",
    "\n",
    "    for query_results, query_passages in zip(rank_eval, rank_content):\n",
    "        if not query_results or not query_passages:\n",
    "            continue\n",
    "\n",
    "        topic_id = query_results[0][\"topic_id\"]\n",
    "        query_text = query_results[0].get(\"description\", topic_id)  # fallback in case description isn't saved\n",
    "\n",
    "        query = Query(query_text)\n",
    "\n",
    "        texts = [Text(text=content, metadata={\"docid\": result[\"doc_id\"]}) \n",
    "                 for content, result in zip([p[1] for p in query_passages], query_results)]\n",
    "\n",
    "        reranked = reranker.rerank(query, texts)\n",
    "\n",
    "        reranked_topic = []\n",
    "        for rank, text in enumerate(reranked):\n",
    "            reranked_topic.append({\n",
    "                \"topic_id\": topic_id,\n",
    "                \"doc_id\": text.metadata[\"docid\"],\n",
    "                \"rank\": rank + 1,\n",
    "                \"score\": text.score\n",
    "            })\n",
    "\n",
    "        reranked_all.append(reranked_topic)\n",
    "\n",
    "    return reranked_all\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
